{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 310019,
     "sourceType": "datasetVersion",
     "datasetId": 129603
    },
    {
     "sourceId": 4484183,
     "sourceType": "datasetVersion",
     "datasetId": 2623949
    }
   ],
   "dockerImageVersionId": 30886,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"datasets/fake_or_real_news.csv\")\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-20T11:20:36.338108Z",
     "iopub.execute_input": "2025-02-20T11:20:36.338675Z",
     "iopub.status.idle": "2025-02-20T11:20:37.002153Z",
     "shell.execute_reply.started": "2025-02-20T11:20:36.338595Z",
     "shell.execute_reply": "2025-02-20T11:20:37.000902Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-20T17:15:28.192106Z",
     "start_time": "2025-02-20T17:15:27.876931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer\nimport re\n\nnltk.download(\"punkt\")\nnltk.download(\"stopwords\")\n\nstop_words = set(stopwords.words(\"english\"))\nstemmer = PorterStemmer()\n\ndef preprocess_text(text):\n    # Küçük harfe dönüştür\n    text = text.lower()\n    # Noktalama işaretlerini kaldır\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Tokenization\n    tokens = word_tokenize(text)\n    # Stop words'leri kaldır ve stemming uygula\n    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n    return ' '.join(tokens)\n\ndf['processed_text'] = df['text'].apply(preprocess_text)",
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-20T11:20:37.004207Z",
     "iopub.execute_input": "2025-02-20T11:20:37.004606Z",
     "iopub.status.idle": "2025-02-20T11:21:30.698290Z",
     "shell.execute_reply.started": "2025-02-20T11:20:37.004577Z",
     "shell.execute_reply": "2025-02-20T11:21:30.696859Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-20T17:15:52.569509Z",
     "start_time": "2025-02-20T17:15:52.219223Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1018)>\n",
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1018)>\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mpunkt_tab\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt_tab/english/\u001B[0m\n\n  Searched in:\n    - '/Users/omerasafkarasu/nltk_data'\n    - '/Users/omerasafkarasu/Desktop/fake-news-checker/.venv/nltk_data'\n    - '/Users/omerasafkarasu/Desktop/fake-news-checker/.venv/share/nltk_data'\n    - '/Users/omerasafkarasu/Desktop/fake-news-checker/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 24\u001B[0m\n\u001B[1;32m     21\u001B[0m     tokens \u001B[38;5;241m=\u001B[39m [stemmer\u001B[38;5;241m.\u001B[39mstem(word) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m tokens \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stop_words]\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(tokens)\n\u001B[0;32m---> 24\u001B[0m df[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprocessed_text\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[43mdf\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreprocess_text\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/fake-news-checker/.venv/lib/python3.13/site-packages/pandas/core/series.py:4924\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001B[0m\n\u001B[1;32m   4789\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4790\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4791\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4796\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4797\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4798\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4799\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4800\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4915\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4916\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m   4917\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSeriesApply\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   4918\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4919\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4920\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconvert_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4921\u001B[0m \u001B[43m        \u001B[49m\u001B[43mby_row\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mby_row\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4922\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   4923\u001B[0m \u001B[43m        \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m-> 4924\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/fake-news-checker/.venv/lib/python3.13/site-packages/pandas/core/apply.py:1427\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_compat()\n\u001B[1;32m   1426\u001B[0m \u001B[38;5;66;03m# self.func is Callable\u001B[39;00m\n\u001B[0;32m-> 1427\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply_standard\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/fake-news-checker/.venv/lib/python3.13/site-packages/pandas/core/apply.py:1507\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1501\u001B[0m \u001B[38;5;66;03m# row-wise access\u001B[39;00m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m \u001B[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001B[39;00m\n\u001B[1;32m   1504\u001B[0m \u001B[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001B[39;00m\n\u001B[1;32m   1505\u001B[0m \u001B[38;5;66;03m#  Categorical (GH51645).\u001B[39;00m\n\u001B[1;32m   1506\u001B[0m action \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj\u001B[38;5;241m.\u001B[39mdtype, CategoricalDtype) \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1507\u001B[0m mapped \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_map_values\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1508\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmapper\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcurried\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconvert_dtype\u001B[49m\n\u001B[1;32m   1509\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1511\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1512\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1513\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1514\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m~/Desktop/fake-news-checker/.venv/lib/python3.13/site-packages/pandas/core/base.py:921\u001B[0m, in \u001B[0;36mIndexOpsMixin._map_values\u001B[0;34m(self, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m    918\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(arr, ExtensionArray):\n\u001B[1;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m arr\u001B[38;5;241m.\u001B[39mmap(mapper, na_action\u001B[38;5;241m=\u001B[39mna_action)\n\u001B[0;32m--> 921\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malgorithms\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mna_action\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mna_action\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/fake-news-checker/.venv/lib/python3.13/site-packages/pandas/core/algorithms.py:1743\u001B[0m, in \u001B[0;36mmap_array\u001B[0;34m(arr, mapper, na_action, convert)\u001B[0m\n\u001B[1;32m   1741\u001B[0m values \u001B[38;5;241m=\u001B[39m arr\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m, copy\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m na_action \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1743\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_infer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalues\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmapper\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconvert\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1744\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1745\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lib\u001B[38;5;241m.\u001B[39mmap_infer_mask(\n\u001B[1;32m   1746\u001B[0m         values, mapper, mask\u001B[38;5;241m=\u001B[39misna(values)\u001B[38;5;241m.\u001B[39mview(np\u001B[38;5;241m.\u001B[39muint8), convert\u001B[38;5;241m=\u001B[39mconvert\n\u001B[1;32m   1747\u001B[0m     )\n",
      "File \u001B[0;32mlib.pyx:2972\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "Cell \u001B[0;32mIn[6], line 19\u001B[0m, in \u001B[0;36mpreprocess_text\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m     17\u001B[0m text \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\u001B[38;5;124mr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m[^\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms]\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m, text)\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Tokenization\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m tokens \u001B[38;5;241m=\u001B[39m \u001B[43mword_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# Stop words'leri kaldır ve stemming uygula\u001B[39;00m\n\u001B[1;32m     21\u001B[0m tokens \u001B[38;5;241m=\u001B[39m [stemmer\u001B[38;5;241m.\u001B[39mstem(word) \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m tokens \u001B[38;5;28;01mif\u001B[39;00m word \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m stop_words]\n",
      "File \u001B[0;32m~/Desktop/fake-news-checker/.venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:142\u001B[0m, in \u001B[0;36mword_tokenize\u001B[0;34m(text, language, preserve_line)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mword_tokenize\u001B[39m(text, language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m\"\u001B[39m, preserve_line\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    128\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;124;03m    Return a tokenized copy of *text*,\u001B[39;00m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;124;03m    using NLTK's recommended word tokenizer\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;124;03m    :type preserve_line: bool\u001B[39;00m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 142\u001B[0m     sentences \u001B[38;5;241m=\u001B[39m [text] \u001B[38;5;28;01mif\u001B[39;00m preserve_line \u001B[38;5;28;01melse\u001B[39;00m \u001B[43msent_tokenize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlanguage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    143\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m    144\u001B[0m         token \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m sentences \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m _treebank_word_tokenizer\u001B[38;5;241m.\u001B[39mtokenize(sent)\n\u001B[1;32m    145\u001B[0m     ]\n",
      "File \u001B[0;32m~/Desktop/fake-news-checker/.venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:119\u001B[0m, in \u001B[0;36msent_tokenize\u001B[0;34m(text, language)\u001B[0m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21msent_tokenize\u001B[39m(text, language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    110\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    111\u001B[0m \u001B[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001B[39;00m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;124;03m    :param language: the model name in the Punkt corpus\u001B[39;00m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 119\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m \u001B[43m_get_punkt_tokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlanguage\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer\u001B[38;5;241m.\u001B[39mtokenize(text)\n",
      "File \u001B[0;32m~/Desktop/fake-news-checker/.venv/lib/python3.13/site-packages/nltk/tokenize/__init__.py:105\u001B[0m, in \u001B[0;36m_get_punkt_tokenizer\u001B[0;34m(language)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mlru_cache\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_get_punkt_tokenizer\u001B[39m(language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     98\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;124;03m    A constructor for the PunktTokenizer that utilizes\u001B[39;00m\n\u001B[1;32m    100\u001B[0m \u001B[38;5;124;03m    a lru cache for performance.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;124;03m    :type language: str\u001B[39;00m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 105\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mPunktTokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlanguage\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/fake-news-checker/.venv/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1744\u001B[0m, in \u001B[0;36mPunktTokenizer.__init__\u001B[0;34m(self, lang)\u001B[0m\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, lang\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   1743\u001B[0m     PunktSentenceTokenizer\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m-> 1744\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_lang\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlang\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/fake-news-checker/.venv/lib/python3.13/site-packages/nltk/tokenize/punkt.py:1749\u001B[0m, in \u001B[0;36mPunktTokenizer.load_lang\u001B[0;34m(self, lang)\u001B[0m\n\u001B[1;32m   1746\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mload_lang\u001B[39m(\u001B[38;5;28mself\u001B[39m, lang\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m   1747\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnltk\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m find\n\u001B[0;32m-> 1749\u001B[0m     lang_dir \u001B[38;5;241m=\u001B[39m \u001B[43mfind\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtokenizers/punkt_tab/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mlang\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1750\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_params \u001B[38;5;241m=\u001B[39m load_punkt_params(lang_dir)\n\u001B[1;32m   1751\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lang \u001B[38;5;241m=\u001B[39m lang\n",
      "File \u001B[0;32m~/Desktop/fake-news-checker/.venv/lib/python3.13/site-packages/nltk/data.py:579\u001B[0m, in \u001B[0;36mfind\u001B[0;34m(resource_name, paths)\u001B[0m\n\u001B[1;32m    577\u001B[0m sep \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m70\u001B[39m\n\u001B[1;32m    578\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 579\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[0;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mpunkt_tab\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt_tab/english/\u001B[0m\n\n  Searched in:\n    - '/Users/omerasafkarasu/nltk_data'\n    - '/Users/omerasafkarasu/Desktop/fake-news-checker/.venv/nltk_data'\n    - '/Users/omerasafkarasu/Desktop/fake-news-checker/.venv/share/nltk_data'\n    - '/Users/omerasafkarasu/Desktop/fake-news-checker/.venv/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "df.head()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-20T11:21:30.700955Z",
     "iopub.execute_input": "2025-02-20T11:21:30.701396Z",
     "iopub.status.idle": "2025-02-20T11:21:30.719243Z",
     "shell.execute_reply.started": "2025-02-20T11:21:30.701363Z",
     "shell.execute_reply": "2025-02-20T11:21:30.717581Z"
    }
   },
   "outputs": [
    {
     "execution_count": 93,
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Unnamed: 0                                              title  \\\n0        8476                       You Can Smell Hillary’s Fear   \n1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n2        3608        Kerry to go to Paris in gesture of sympathy   \n3       10142  Bernie supporters on Twitter erupt in anger ag...   \n4         875   The Battle of New York: Why This Primary Matters   \n\n                                                text label  \\\n0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE   \n1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE   \n2  U.S. Secretary of State John F. Kerry said Mon...  REAL   \n3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE   \n4  It's primary day in New York and front-runners...  REAL   \n\n                                      processed_text  \n0  daniel greenfield shillman journal fellow free...  \n1  googl pinterest digg linkedin reddit stumbleup...  \n2  us secretari state john f kerri said monday st...  \n3  kayde king kaydeek novemb 9 2016 lesson tonigh...  \n4  primari day new york frontrunn hillari clinton...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>title</th>\n      <th>text</th>\n      <th>label</th>\n      <th>processed_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8476</td>\n      <td>You Can Smell Hillary’s Fear</td>\n      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n      <td>FAKE</td>\n      <td>daniel greenfield shillman journal fellow free...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10294</td>\n      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n      <td>FAKE</td>\n      <td>googl pinterest digg linkedin reddit stumbleup...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3608</td>\n      <td>Kerry to go to Paris in gesture of sympathy</td>\n      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n      <td>REAL</td>\n      <td>us secretari state john f kerri said monday st...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10142</td>\n      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n      <td>FAKE</td>\n      <td>kayde king kaydeek novemb 9 2016 lesson tonigh...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>875</td>\n      <td>The Battle of New York: Why This Primary Matters</td>\n      <td>It's primary day in New York and front-runners...</td>\n      <td>REAL</td>\n      <td>primari day new york frontrunn hillari clinton...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 93
  },
  {
   "cell_type": "code",
   "source": "from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vectorizer = TfidfVectorizer(max_features = 5000)\nx = tfidf_vectorizer.fit_transform(df[\"processed_text\"]).toarray()\ny = df[\"label\"].values",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-20T11:21:30.721123Z",
     "iopub.execute_input": "2025-02-20T11:21:30.721549Z",
     "iopub.status.idle": "2025-02-20T11:21:34.905591Z",
     "shell.execute_reply.started": "2025-02-20T11:21:30.721508Z",
     "shell.execute_reply": "2025-02-20T11:21:34.904396Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-20T17:15:09.566675Z",
     "start_time": "2025-02-20T17:15:09.532611Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_extraction\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtext\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m TfidfVectorizer\n\u001B[1;32m      3\u001B[0m tfidf_vectorizer \u001B[38;5;241m=\u001B[39m TfidfVectorizer(max_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5000\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m x \u001B[38;5;241m=\u001B[39m tfidf_vectorizer\u001B[38;5;241m.\u001B[39mfit_transform(\u001B[43mdf\u001B[49m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprocessed_text\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mtoarray()\n\u001B[1;32m      5\u001B[0m y \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabel\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mvalues\n",
      "\u001B[0;31mNameError\u001B[0m: name 'df' is not defined"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\n\n# Veriyi eğitim ve test setlerine ayır\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n\n# Logistic Regression modeli\n\nmodel = LogisticRegression()\n#model = GaussianNB()\nmodel.fit(x_train, y_train)\n\n# Tahmin yap\ny_pred = model.predict(x_test)\nprint(\"Tahmin: \", y_pred)\n\n# Doğruluk değerlendirmesi\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Model Doğruluğu: {accuracy * 100:.2f}%')",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-20T11:24:00.701475Z",
     "iopub.execute_input": "2025-02-20T11:24:00.701807Z",
     "iopub.status.idle": "2025-02-20T11:24:01.809335Z",
     "shell.execute_reply.started": "2025-02-20T11:24:00.701787Z",
     "shell.execute_reply": "2025-02-20T11:24:01.808036Z"
    },
    "ExecuteTime": {
     "end_time": "2025-02-20T17:08:34.947955Z",
     "start_time": "2025-02-20T17:08:13.031090Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnaive_bayes\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m GaussianNB\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# Veriyi eğitim ve test setlerine ayır\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m x_train, x_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(\u001B[43mx\u001B[49m, y, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Logistic Regression modeli\u001B[39;00m\n\u001B[1;32m     11\u001B[0m model \u001B[38;5;241m=\u001B[39m LogisticRegression()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'x' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "import joblib\n\n# Modeli kaydet\njoblib.dump(model, 'fake_news_model.pkl')\n\n# Modeli yükle\nmodel = joblib.load('fake_news_model.pkl')\n\ndef ask(text):\n    processed_text = preprocess_text(text)\n    vectorized_text = tfidf_vectorizer.transform([processed_text]).toarray()\n    prediction = model.predict(vectorized_text)\n    print(prediction)\n    print('Gerçek Haber' if prediction[0] == \"REAL\" else 'Sahte Haber')",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-20T11:23:38.133981Z",
     "iopub.execute_input": "2025-02-20T11:23:38.134402Z",
     "iopub.status.idle": "2025-02-20T11:23:38.144463Z",
     "shell.execute_reply.started": "2025-02-20T11:23:38.134369Z",
     "shell.execute_reply": "2025-02-20T11:23:38.143447Z"
    }
   },
   "outputs": [],
   "execution_count": 121
  },
  {
   "cell_type": "code",
   "source": "import random\nnum = random.randint(0, 100)\n\ntest = df[\"text\"][num]\nprint(df[\"label\"][num])\nask(test)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-20T11:30:32.621441Z",
     "iopub.execute_input": "2025-02-20T11:30:32.621800Z",
     "iopub.status.idle": "2025-02-20T11:30:32.631743Z",
     "shell.execute_reply.started": "2025-02-20T11:30:32.621771Z",
     "shell.execute_reply": "2025-02-20T11:30:32.630226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "FAKE\n['FAKE']\nSahte Haber\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 158
  },
  {
   "cell_type": "code",
   "source": "ask(\"Harry potter is dead\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-20T11:27:21.266111Z",
     "iopub.execute_input": "2025-02-20T11:27:21.266491Z",
     "iopub.status.idle": "2025-02-20T11:27:21.273386Z",
     "shell.execute_reply.started": "2025-02-20T11:27:21.266463Z",
     "shell.execute_reply": "2025-02-20T11:27:21.272495Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "['FAKE']\nSahte Haber\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 133
  },
  {
   "cell_type": "code",
   "source": "ask(\"Harry Potter is live\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-20T11:27:34.228758Z",
     "iopub.execute_input": "2025-02-20T11:27:34.229085Z",
     "iopub.status.idle": "2025-02-20T11:27:34.237080Z",
     "shell.execute_reply.started": "2025-02-20T11:27:34.229060Z",
     "shell.execute_reply": "2025-02-20T11:27:34.235496Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "['FAKE']\nSahte Haber\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 135
  },
  {
   "cell_type": "code",
   "source": "ask(\"Biden is live\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-20T11:27:48.623418Z",
     "iopub.execute_input": "2025-02-20T11:27:48.623750Z",
     "iopub.status.idle": "2025-02-20T11:27:48.629891Z",
     "shell.execute_reply.started": "2025-02-20T11:27:48.623724Z",
     "shell.execute_reply": "2025-02-20T11:27:48.629106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "['FAKE']\nSahte Haber\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 136
  },
  {
   "cell_type": "code",
   "source": "ask(\"Trump is a president of America\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-20T11:28:02.843456Z",
     "iopub.execute_input": "2025-02-20T11:28:02.843759Z",
     "iopub.status.idle": "2025-02-20T11:28:02.851100Z",
     "shell.execute_reply.started": "2025-02-20T11:28:02.843740Z",
     "shell.execute_reply": "2025-02-20T11:28:02.850094Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "['FAKE']\nSahte Haber\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 137
  },
  {
   "cell_type": "code",
   "source": "ask(\"\"\"Sir Keir called Zelensky on Wednesday evening and told him it was \"perfectly reasonable\" for Ukraine to \"suspend elections during wartime as the UK did during World War Two\", Downing Street said. The US president had earlier criticised Zelensky, saying he had done a \"terrible job\" and claiming \"he refuses to have elections\" in Ukraine. Zelensky's five-year term was due to end in May 2024, but elections have been suspended since martial law was declared after Russia's invasion.\"\"\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-20T11:30:03.452396Z",
     "iopub.execute_input": "2025-02-20T11:30:03.452798Z",
     "iopub.status.idle": "2025-02-20T11:30:03.463069Z",
     "shell.execute_reply.started": "2025-02-20T11:30:03.452767Z",
     "shell.execute_reply": "2025-02-20T11:30:03.461799Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "['FAKE']\nSahte Haber\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 143
  },
  {
   "cell_type": "code",
   "source": "ask(\" wilamid\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-02-20T11:31:14.017981Z",
     "iopub.execute_input": "2025-02-20T11:31:14.018350Z",
     "iopub.status.idle": "2025-02-20T11:31:14.025836Z",
     "shell.execute_reply.started": "2025-02-20T11:31:14.018298Z",
     "shell.execute_reply": "2025-02-20T11:31:14.024597Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "['FAKE']\nSahte Haber\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 161
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
